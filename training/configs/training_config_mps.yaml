# MPS-optimized configuration for M4 Max
data_path: "data/inputs/train.json"
validation_data_path: "data/inputs/validation.json"
output_dir: "./outputs/qwen-0.5b-mps"

# Reduced settings for MPS
num_epochs: 3
batch_size: 1  # Must be 1 for MPS
learning_rate: 2e-5
weight_decay: 0.01
warmup_steps: 50
gradient_accumulation_steps: 8  # Compensate for small batch

# MPS-specific limits
max_sequence_length: 512  # Reduced from 2048
max_new_tokens: 256  # Limit generation length

# Memory optimization
gradient_checkpointing: true
clear_cache_steps: 10  # Clear more frequently
eval_batch_size: 1
eval_steps: 100

# Disable features that don't work on MPS
fp16: false  # Use fp32 on MPS
use_8bit_adam: false  # Not supported on MPS

device_config:
  use_mps: true
  use_cuda: false
  device_map: null  # Don't use device_map on MPS

# LoRA mode settings (for MPS)
lora_mode:
  per_device_train_batch_size: 1
  gradient_checkpointing: true
  fp16: false
  
full_finetune_mode:
  per_device_train_batch_size: 1
  gradient_checkpointing: true
  bf16: false
  deepspeed_config: null

# Evaluation settings
eval_steps: 100
eval_batch_size: 1
save_steps: 200
save_total_limit: 3
load_best_model_at_end: true
metric_for_best_model: "eval_success_rate"
greater_is_better: true

# Early stopping
early_stopping:
  enabled: true
  patience: 5
  threshold: 0.001

# Logging configuration
logging:
  logging_steps: 10
  logging_first_step: true
  report_to: ["wandb", "weave"]
  wandb_project: "skyrl-qwen-mps"
  weave_project: "synergia_Agents/skyrl-qwen-mps"
  
# Curriculum learning settings
curriculum_learning:
  enabled: true
  warmup_epochs: 1
  difficulty_schedule: "linear"
  min_complexity: "easy"
  max_complexity: "hard"
  
# Data loader settings
cache_size: 100
num_workers: 0  # Single thread for MPS
shuffle: true
seed: 42

# Memory optimization
memory_optimization:
  gradient_checkpointing: true
  clear_cache_steps: 10
  max_memory_mb: 8000  # 8GB limit