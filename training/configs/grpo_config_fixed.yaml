# GRPO Algorithm Configuration - Fixed Reference Policy Updates
algorithm: "grpo"                    # Group Relative Policy Optimization
algorithm_version: "1.1"             # Version with fixed reference policy updates

# GRPO Core Parameters
group_size: 4                        # Increased for better GPU utilization
kl_penalty_coef: 0.01               # Reduced from 0.1 to prevent gradient explosion
target_kl_divergence: 0.01          # Target KL for adaptive penalty adjustment
adaptive_kl_penalty: true           # Adapt KL penalty based on actual KL
kl_penalty_warmup_steps: 100        # Reduced warmup for faster adaptation

# Generalized Advantage Estimation (GAE)
gamma: 0.99                          # Discount factor for future rewards
gae_lambda: 0.95                     # GAE lambda parameter for bias-variance tradeoff
normalize_advantages: true           # Normalize advantages to have zero mean, unit variance
normalize_rewards: true              # Normalize rewards across rollouts
advantage_epsilon: 1e-8              # Small epsilon for advantage normalization

# Policy Gradient Settings (PPO-style clipping)
clip_ratio: 0.2                      # PPO clipping parameter
entropy_coef: 0.01                   # Entropy regularization coefficient
value_loss_coef: 0.5                # Value function loss coefficient

# Reference Policy Management - FIXED SETTINGS
ref_policy_update_frequency: 100     # Update every 100 steps (was 10000!)
ref_policy_ema_alpha: 0.95          # Faster updates (was 0.99)
ref_policy_init_from_checkpoint: null
emergency_kl_threshold: 0.05         # Force update if KL exceeds this
min_steps_before_ref_update: 10      # Don't update in first 10 steps

# Value Function Settings (NEW)
clip_value_loss: true               # PPO-style value clipping
value_clip_range: 0.2               # Clipping range for value predictions

# Reward Processing
reward_scaling: 1.0                  # Scale all rewards by this factor
reward_clipping_value: 5.0           # Clip rewards to [-value, value]
reward_clipping_enabled: true        # Enable reward clipping
reward_normalization: "group"        # Normalize rewards within group
reward_normalization_eps: 1e-8       # Epsilon for reward normalization

# Episode and Environment Settings
max_episode_length: 15               # Maximum turns per episode
episode_timeout_penalty: -1.0       # Penalty for exceeding max length
early_termination_reward: 0.1       # Small reward for successful early termination

# Rollout Collection Settings
rollout_parallel_envs: 8             # Increased for parallel trajectory collection
rollout_batch_size: 16               # Increased for A100 GPU utilization
episodes_per_update: 8               # Increased for more training data
max_rollout_length: 20               # Increased for longer trajectories
rollout_timeout_seconds: 300         # Timeout for rollout collection (5 minutes)

# Training Stability and Regularization
gradient_clipping_enabled: true      # Enable gradient clipping
gradient_clipping_value: 1.0         # Gradient clipping threshold
max_grad_norm: 1.0                  # Maximum gradient norm (ADDED)
policy_loss_clipping: 10.0          # Clip policy loss to prevent instability
value_loss_clipping: 10.0           # Clip value loss to prevent instability

# NEW: Advanced Stability Controls (Mathematically Grounded)
kl_target: 0.4                      # Target KL per token (nats)
kl_penalty_coef: 0.2                # KL penalty coefficient (adaptive)
kl_hard_cap: 3.0                    # Skip update if KL exceeds this (production value)
min_unforced_tokens_per_step: 0     # Allow bootstrap training on forced tokens (was 1)
min_unforced_fraction_per_step: 0.0  # Allow any fraction during bootstrap (was 0.10)
min_ratio_per_token: 0.25           # Drop tokens with PPO ratio < this value
max_ratio_per_token: 4.0            # Drop tokens with PPO ratio > this value
adaptive_kl_beta: true              # Enable adaptive KL coefficient
kl_tolerance: 0.15                  # Tolerance band for adaptive KL (15%)

# Numerical Stability (NEW)
ratio_eps: 1e-8                     # Epsilon for ratio computation
log_prob_eps: 1e-8                  # Epsilon for log probability computation

# Trust Region Constraints
trust_region_enabled: false         # Enable trust region constraints
trust_region_threshold: 0.01        # Trust region threshold for policy updates
trust_region_decay: 0.99            # Decay trust region over time

# Multi-turn Specific Settings
turn_level_rewards: true            # Provide rewards at each turn
turn_completion_bonus: 0.1          # Bonus for completing turns successfully
reasoning_quality_weight: 0.3       # Weight for reasoning quality in rewards
tool_efficiency_weight: 0.2         # Weight for tool use efficiency
task_completion_weight: 0.5         # Weight for overall task completion

# Advanced GRPO Settings
use_importance_sampling: true       # Use importance sampling for off-policy correction
importance_sampling_clip: 2.0       # Clip importance sampling ratios
baseline_subtraction: true          # Subtract baseline from rewards
baseline_ema_alpha: 0.95            # EMA for baseline estimation

# Loss Function Weights and Scheduling
policy_loss_weight: 1.0             # Weight for policy loss component
value_loss_weight: 0.5              # Weight for value loss component
entropy_loss_weight: 0.01           # Weight for entropy regularization
kl_loss_weight: 0.1                 # Weight for KL divergence penalty

# Learning Rate Scheduling for GRPO Components
policy_lr_schedule: "constant"       # Learning rate schedule for policy
value_lr_schedule: "constant"        # Learning rate schedule for value function
entropy_schedule: "constant"         # Entropy coefficient schedule

# Logging and Monitoring
log_probabilities: true              # Log action probabilities
log_advantages: true                 # Log computed advantages
log_rewards_breakdown: true          # Log detailed reward breakdown
log_kl_divergence: true             # Log KL divergence from reference
log_episode_statistics: true        # Log episode-level statistics
log_turn_level_metrics: true        # Log turn-level performance metrics
log_reference_policy_updates: true   # Log when reference policy is updated (NEW)

# Debugging and Analysis
save_rollout_data: false            # Save rollout data for analysis
rollout_save_frequency: 1000        # Save rollouts every N steps
analyze_policy_updates: true        # Analyze policy update statistics
track_parameter_norms: true         # Track model parameter norms
debug_reference_policy: true        # Extra logging for reference policy (NEW)

# Performance Monitoring
monitor_memory_usage: true          # Monitor GPU/CPU memory usage
monitor_rollout_time: true          # Monitor time for rollout collection
monitor_training_time: true         # Monitor training step time
performance_log_frequency: 10       # Log performance metrics every N steps (reduced)

# Tool Use Specific Rewards
tool_selection_accuracy_weight: 0.2  # Weight for correct tool selection
tool_parameter_accuracy_weight: 0.2  # Weight for correct tool parameters
tool_sequence_efficiency_weight: 0.1 # Weight for efficient tool sequences
reasoning_coherence_weight: 0.2      # Weight for coherent reasoning
final_answer_quality_weight: 0.3     # Weight for final answer quality