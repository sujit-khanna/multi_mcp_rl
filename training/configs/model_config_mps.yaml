# Model configuration for MPS
model_name: "Qwen/Qwen2.5-0.5B-Instruct"
tokenizer_name: "Qwen/Qwen2.5-0.5B-Instruct"
model_revision: "main"
trust_remote_code: true

# Reduced limits for MPS
max_length: 512  # Reduced from 2048/4096
max_position_embeddings: 512
context_length: 512
vocab_size: 151936
hidden_size: 896
num_attention_heads: 14
num_hidden_layers: 24

# Tokenizer settings
padding_side: "left"
truncation_side: "left"
add_special_tokens: true
use_fast_tokenizer: true

# LoRA configuration (optimized for MPS)
lora_mode:
  enabled: true
  r: 16  # Reduced rank
  alpha: 32
  target_modules: ["q_proj", "v_proj"]  # Fewer modules for memory
  dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"
  fan_in_fan_out: false
  init_lora_weights: true

# Generation settings
generation:
  max_new_tokens: 256
  temperature: 0.7
  do_sample: true
  top_p: 0.9
  top_k: 50
  pad_token_id: 151643
  eos_token_id: 151645

# Stop sequences
stop_sequences:
  - "</tool_call>"
  - "</think>"
  - "</s>"
  - "<|im_end|>"

# Full fine-tuning (disabled for MPS)
full_finetune_mode:
  enabled: false
  gradient_checkpointing: false  # DISABLED: Incompatible with Qwen2 KV caching
  use_flash_attention: false
  bf16: false
  fp16: false
  torch_compile: false

# Memory optimization settings (MPS-specific)
memory_optimization:
  low_cpu_mem_usage: true
  device_map: null  # Don't use device_map on MPS
  torch_dtype: "float32"  # MPS works better with fp32
  attn_implementation: null  # No flash attention on MPS

# Model loading optimization
loading:
  use_safetensors: true
  local_files_only: false
  cache_dir: null
  revision: "main"

# Quantization settings (disabled for MPS)
quantization:
  load_in_4bit: false  # Not supported on MPS
  load_in_8bit: false  # Not supported on MPS
  bnb_4bit_compute_dtype: "float32"
  bnb_4bit_use_double_quant: false
  bnb_4bit_quant_type: "nf4"

# Training-specific settings
training:
  gradient_checkpointing: false  # DISABLED: Incompatible with Qwen2 KV caching
  optim: "adamw_torch"  # Standard AdamW for MPS
  use_lora: true
  lora_rank: 16
  lora_alpha: 32
  lora_dropout: 0.05