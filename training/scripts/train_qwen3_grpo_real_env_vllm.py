#!/usr/bin/env python3
"""
vLLM-Enhanced GRPO Training Script with REAL Environment Rollouts
================================================================

This script integrates vLLM for 10x faster inference while maintaining
all the fixes for real environment training. Key features:

- vLLM for ultra-fast generation (10x speedup)
- Real MCPToolEnvironment for actual tool execution  
- TrajectoryCollector for parallel rollout collection
- All critical fixes from the original script
- Memory-optimized for high GPU utilization (70-90%)
"""

import argparse
import copy
import json
import logging
import os
import sys
import time
import asyncio
from pathlib import Path
from typing import Dict, List, Any, Optional

import torch
import yaml
import numpy as np
from tqdm import tqdm

# Add parent directories to path
sys.path.append(str(Path(__file__).parent.parent))
sys.path.append(str(Path(__file__).parent.parent.parent))

# Add utils path for tool validation
utils_path = str(Path(__file__).parent.parent / "utils")
if utils_path not in sys.path:
    sys.path.insert(0, utils_path)

# Check for vLLM availability
VLLM_AVAILABLE = False
try:
    import vllm
    from vllm import LLM, SamplingParams
    VLLM_AVAILABLE = True
    print(f"‚úÖ vLLM {vllm.__version__} detected - enabling 10x faster inference")
except ImportError:
    print("‚ö†Ô∏è  vLLM not available - falling back to HuggingFace (slower)")

# Import our enhanced components  
from core.qwen_policy_with_value_prompting import QwenPolicyWithValuePrompting
from core.grpo_trainer_gradient_fix import GRPOTrainerGradientFix
from core.grpo_trainer import Trajectory

# Import data components
from data.trajectory_collector import TrajectoryCollector, EpisodeResult

# Import environment components
env_path = str(Path(__file__).parent.parent.parent / "environments")
if env_path not in sys.path:
    sys.path.insert(0, env_path)

from mcp_tool_environment import MCPToolEnvironment
from simple_shared_manager import SimpleSharedManager

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
    ]
)
logger = logging.getLogger(__name__)


class VLLMQwenPolicy:
    """vLLM-enhanced policy for ultra-fast generation"""
    
    def __init__(self, model_name="Qwen/Qwen2.5-0.5B-Instruct", **kwargs):
        self.model_name = model_name
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Add required attributes for GRPO trainer compatibility
        self.use_lora = False  # We're using full model, not LoRA
        self.model = None  # Will be set after initialization
        
        # Initialize vLLM if available
        if VLLM_AVAILABLE and os.getenv("ENABLE_VLLM", "false").lower() == "true":
            self.use_vllm = True
            self._init_vllm()
        else:
            self.use_vllm = False
            self._init_hf()
            
        logger.info(f"üöÄ Policy initialized - vLLM: {self.use_vllm}")
    
    def get_trainable_parameters(self):
        """Get trainable parameters for GRPO trainer compatibility"""
        trainable_params = list(self.parameters())
        count = sum(p.numel() for p in trainable_params)
        logger.info(f"üîß get_trainable_parameters() returning {len(trainable_params)} parameters ({count} total elements)")
        return trainable_params
    
    def compute_values(self, input_ids, attention_mask=None):
        """Compute value estimates for GRPO training"""
        # Handle case where input_ids might be a list of states instead of tensor
        if not isinstance(input_ids, torch.Tensor):
            # If input_ids is a list of states, we need to tokenize them first
            if isinstance(input_ids, list):
                states = input_ids
                values = []
                for state in states:
                    # Convert state to string if needed
                    if isinstance(state, dict) or isinstance(state, list):
                        state_str = str(state)
                    else:
                        state_str = str(state)
                    
                    # Tokenize the state
                    tokens = self.tokenizer(state_str, return_tensors="pt", truncation=True, max_length=1500, padding=True)
                    tokens = {k: v.to(self.device) for k, v in tokens.items()}
                    
                    # Get value for this state (PRESERVE GRADIENTS FOR TRAINING)
                    if hasattr(self, 'value_head') and self.value_head is not None:
                        hidden_states = self.training_model(**tokens, output_hidden_states=True).hidden_states[-1]
                        # Use last token's hidden state and convert to FP32 for value head
                        last_hidden = hidden_states[:, -1, :].float()
                        value = self.value_head(last_hidden)
                        values.append(value.squeeze())
                    else:
                        values.append(torch.tensor(0.0, device=self.device, requires_grad=True))
                
                return torch.stack(values) if values else torch.tensor([], device=self.device)
        
        # Original tensor-based computation (PRESERVE GRADIENTS for training)
        if hasattr(self, 'value_head') and self.value_head is not None:
            # Get hidden states from the training model (not combined model)
            outputs = self.training_model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)
            hidden_states = outputs.hidden_states[-1]  # Last layer hidden states
                
            # Convert to FP32 for value head to avoid numerical instability
            hidden_states_fp32 = hidden_states.float()
            
            # Compute values using the value head
            values = self.value_head(hidden_states_fp32)
            return values.squeeze(-1)  # Remove last dimension
        else:
            # Return zeros if no value head available (with gradients enabled)
            if hasattr(input_ids, 'shape'):
                batch_size, seq_len = input_ids.shape
                return torch.zeros(batch_size, seq_len, device=input_ids.device, requires_grad=True)
            else:
                return torch.tensor(0.0, device=self.device, requires_grad=True)
    
    def enable_eval_mode(self):
        """Enable evaluation mode for trajectory collection"""
        if hasattr(self, 'model') and self.model is not None:
            self.model.eval()
        if hasattr(self, 'training_model') and self.training_model is not None:
            self.training_model.eval()
        if hasattr(self, 'value_head') and self.value_head is not None:
            self.value_head.eval()
    
    def enable_train_mode(self):
        """Enable training mode"""
        if hasattr(self, 'model') and self.model is not None:
            self.model.train()
        if hasattr(self, 'training_model') and self.training_model is not None:
            self.training_model.train()
        if hasattr(self, 'value_head') and self.value_head is not None:
            self.value_head.train()
    
    def enable_training_mode(self):
        """Enable training mode (alias for compatibility)"""
        self.enable_train_mode()
    
    def generate_action(self, states, max_new_tokens=None, temperature=None, **kwargs):
        """Generate actions for trajectory collection - core method needed by TrajectoryCollector"""
        if not isinstance(states, list):
            states = [states]
            
        # Convert conversation states to text prompts
        formatted_inputs = []
        for state in states:
            if isinstance(state, list):
                # Convert conversation history to text
                conversation_text = ""
                for msg in state:
                    role = msg.get("role", "user")
                    content = msg.get("content", "")
                    conversation_text += f"{role}: {content}\n"
                conversation_text += "assistant: "
                formatted_inputs.append(conversation_text)
            else:
                formatted_inputs.append(str(state))
        
        # Use vLLM or HuggingFace for generation
        responses = self.generate(
            formatted_inputs, 
            max_tokens=max_new_tokens or 512,
            temperature=temperature or 0.1
        )
        
        # Handle empty responses
        processed_responses = []
        for response in responses:
            if not response or len(response.strip()) == 0:
                # Generate fallback tool call if empty
                logger.warning("Empty response generated, using fallback tool call")
                fallback = '<tool_call>{"name": "fmp_get_quote", "arguments": {"symbol": "AAPL"}}</tool_call>'
                processed_responses.append(fallback)
            else:
                processed_responses.append(response)
        
        return processed_responses
    
    def compute_log_probs(self, states, actions, **kwargs):
        """Compute log probabilities for given state-action pairs"""
        if not hasattr(self, 'training_model') or self.training_model is None:
            raise RuntimeError("No training model available for log prob computation")
        
        # CRITICAL: Ensure model is in training mode for gradient computation
        if not self.training_model.training:
            logger.warning("‚ö†Ô∏è Model was in eval mode during compute_log_probs, switching to train mode")
            self.training_model.train()
            self.value_head.train()
            
        # Ensure states and actions are lists
        if not isinstance(states, list):
            states = [states]
        if not isinstance(actions, list):
            actions = [actions]
        
        # Prepare batch inputs for efficient computation
        batch_prompts = []
        batch_targets = []
        
        for state, action in zip(states, actions):
            # Convert state to prompt
            if isinstance(state, list):
                conversation_text = ""
                for msg in state:
                    role = msg.get("role", "user")
                    content = msg.get("content", "")
                    conversation_text += f"{role}: {content}\n"
                prompt = conversation_text + "assistant: "
            else:
                prompt = str(state)
            
            # Ensure action is a string and not empty
            action_str = str(action) if action else " "  # Use space if empty
            
            batch_prompts.append(prompt)
            batch_targets.append(action_str)
        
        # Tokenize all prompts and targets
        batch_log_probs = []
        
        for prompt, target in zip(batch_prompts, batch_targets):
            # Tokenize prompt and full sequence
            prompt_tokens = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1500)
            full_text = prompt + target
            full_tokens = self.tokenizer(full_text, return_tensors="pt", truncation=True, max_length=2000)
            
            # Move to device
            prompt_tokens = {k: v.to(self.device) for k, v in prompt_tokens.items()}
            full_tokens = {k: v.to(self.device) for k, v in full_tokens.items()}
            
            # Get target token positions
            prompt_len = prompt_tokens["input_ids"].shape[1]
            target_tokens = full_tokens["input_ids"][:, prompt_len:]
            
            # Skip if no target tokens (shouldn't happen with our fallback)
            if target_tokens.shape[1] == 0:
                # Force at least one token by tokenizing a single space
                space_tokens = self.tokenizer(" ", return_tensors="pt")["input_ids"].to(self.device)
                target_tokens = space_tokens[:, -1:]  # Take last token
            
            # Compute logits for the sequence (excluding last token for autoregressive prediction)
            input_ids = full_tokens["input_ids"][:, :-1]
            
            # CRITICAL: Ensure gradients are enabled for the forward pass
            # This must NOT be in a torch.no_grad() context
            outputs = self.training_model(input_ids=input_ids)
            logits = outputs.logits
            
            # Note: With 4-bit quantization, logits won't have requires_grad=True directly
            # because the base model is frozen. Only LoRA adapters are trainable.
            # The gradient will flow through the LoRA adapters when we compute the loss.
            
            # Get logits for target positions
            target_len = target_tokens.shape[1]
            if prompt_len > 0 and prompt_len + target_len - 1 <= logits.shape[1]:
                # Extract logits corresponding to target tokens
                target_logits = logits[:, prompt_len-1:prompt_len+target_len-1, :]
                
                # MEMORY-EFFICIENT LOG PROB COMPUTATION: Use gather instead of log_softmax
                # This avoids creating large [batch, seq, vocab] tensors
                if target_logits.shape[1] == target_tokens.shape[1]:
                    # Gather logits for target tokens: [batch, seq, 1] -> [batch, seq]
                    token_logits = target_logits.gather(2, target_tokens.unsqueeze(-1)).squeeze(-1)
                    # Compute log normalization: [batch, seq]
                    log_normalizers = target_logits.logsumexp(dim=-1)
                    # Log probabilities = token_logits - log_normalizers
                    token_log_probs = token_logits - log_normalizers
                    total_log_prob = token_log_probs.sum()
                else:
                    # Compute what we can if there's a mismatch
                    min_len = min(target_logits.shape[1], target_tokens.shape[1])
                    if min_len > 0:
                        target_subset = target_tokens[:, :min_len]
                        logits_subset = target_logits[:, :min_len, :]
                        # Memory-efficient log prob computation
                        token_logits = logits_subset.gather(2, target_subset.unsqueeze(-1)).squeeze(-1)
                        log_normalizers = logits_subset.logsumexp(dim=-1)
                        token_log_probs = token_logits - log_normalizers
                        total_log_prob = token_log_probs.sum()
                    else:
                        # Last resort: compute log prob for a single space token
                        space_id = self.tokenizer(" ", return_tensors="pt")["input_ids"][0, -1].to(self.device)
                        space_logits = logits[:, -1, :]  # Last position
                        # Memory-efficient computation
                        space_token_logit = space_logits[0, space_id]
                        log_normalizer = space_logits.logsumexp(dim=-1)[0]
                        total_log_prob = space_token_logit - log_normalizer
            else:
                # Fallback: compute log prob for end token at last position
                eos_id = self.tokenizer.eos_token_id or self.tokenizer.pad_token_id
                if eos_id is not None:
                    last_logits = logits[:, -1, :]
                    # Memory-efficient computation
                    eos_token_logit = last_logits[0, eos_id]
                    log_normalizer = last_logits.logsumexp(dim=-1)[0]
                    total_log_prob = eos_token_logit - log_normalizer
                else:
                    # Very last resort: Use a small value from the logits to maintain gradient connection
                    # We take the mean of logits to keep it connected to the computation graph
                    total_log_prob = logits.mean() * 0.0  # Multiply by 0 but keep gradient connection
            
            batch_log_probs.append(total_log_prob)
        
        # Stack log probs and ensure they maintain gradient connection
        stacked_log_probs = torch.stack(batch_log_probs)
        
        # For 4-bit models, the gradient flows through LoRA adapters
        # The stacked log probs should maintain the computation graph
        return stacked_log_probs
    
    def _init_vllm(self):
        """Initialize vLLM for fast generation"""
        try:
            logger.info("üî• Initializing vLLM for ultra-fast inference...")
            
            # Ensure we're using the correct context length from environment
            max_model_len = int(os.getenv("VLLM_MAX_MODEL_LEN", "4096"))
            logger.info(f"üîß Setting vLLM max_model_len to {max_model_len}")
            
            self.vllm_engine = LLM(
                model=self.model_name,
                max_model_len=max_model_len,
                gpu_memory_utilization=float(os.getenv("VLLM_GPU_MEMORY_UTILIZATION", "0.6")),
                enforce_eager=True,  # Disable CUDA graphs for training compatibility
                trust_remote_code=True,
                dtype="auto",
                quantization=None,  # Let vLLM handle optimization
            )
            
            # Initialize HuggingFace components for training (value head, LoRA)
            self._init_hf_training_components()
            
            logger.info("‚úÖ vLLM engine initialized successfully")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  vLLM initialization failed: {e}")
            logger.info("üîÑ Falling back to HuggingFace...")
            self.use_vllm = False
            self._init_hf()
    
    def _init_hf_training_components(self):
        """Initialize HuggingFace components needed for training"""
        from transformers import AutoTokenizer, AutoModelForCausalLM
        from peft import LoraConfig, get_peft_model
        
        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # Load model for training (with LoRA)
        self.training_model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            load_in_4bit=True,
            trust_remote_code=True
        )
        
        # Apply LoRA for training
        lora_config = LoraConfig(
            r=8,
            lora_alpha=32,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
            lora_dropout=0.1,
            bias="none",
            task_type="CAUSAL_LM",
        )
        self.training_model = get_peft_model(self.training_model, lora_config)
        
        # Ensure all LoRA parameters require gradients
        for name, param in self.training_model.named_parameters():
            if "lora_" in name or param.requires_grad:
                param.requires_grad = True
                logger.debug(f"Enabled gradients for: {name}")
        
        # Add value head
        self._add_value_head()
        
        # Create a wrapper model that includes both training_model and value_head
        self.model = self._create_combined_model()
        
        # CRITICAL: Ensure training model is in training mode and parameters require gradients
        self.training_model.train()
        self.value_head.train()
        
        # Double-check LoRA parameters still require gradients after all initialization
        lora_count = 0
        for name, param in self.training_model.named_parameters():
            if "lora_" in name:
                if not param.requires_grad:
                    logger.warning(f"‚ö†Ô∏è Forcing LoRA parameter {name} to require gradients")
                    param.requires_grad = True
                lora_count += 1
        logger.info(f"‚úÖ Verified {lora_count} LoRA parameters require gradients")
        
    def _add_value_head(self):
        """Add value head for GRPO training"""
        import torch.nn as nn
        
        hidden_size = self.training_model.config.hidden_size
        self.value_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, 1)
        ).to(self.device).float()  # Use FP32 for value head to avoid numerical instability
        
        # Ensure all value head parameters require gradients
        for param in self.value_head.parameters():
            param.requires_grad = True
        
        logger.info(f"‚úÖ Added value head to model (fp32 for numerical stability, gradients enabled)")
    
    def _create_combined_model(self):
        """Create a wrapper model that includes both training_model and value_head parameters"""
        import torch.nn as nn
        
        class CombinedModel(nn.Module):
            def __init__(self, training_model, value_head):
                super().__init__()
                # CRITICAL FIX: Register submodules properly using add_module
                # This ensures PyTorch's parameter tracking works correctly
                self.add_module("training_model", training_model)
                self.add_module("value_head", value_head)
                self.training = True  # Ensure training state is preserved
                
            def parameters(self, recurse=True):
                # Return parameters from both training_model and value_head
                for param in self.training_model.parameters(recurse=recurse):
                    yield param
                for param in self.value_head.parameters(recurse=recurse):
                    yield param
                    
            def train(self, mode=True):
                self.training = mode
                self.training_model.train(mode)
                self.value_head.train(mode)
                return self
                
            def eval(self):
                return self.train(False)
        
        combined = CombinedModel(self.training_model, self.value_head)
        logger.info(f"‚úÖ Created combined model with {sum(1 for _ in combined.parameters())} total parameters")
        return combined
    
    def _init_hf(self):
        """Initialize HuggingFace for generation and training"""
        from transformers import AutoTokenizer, AutoModelForCausalLM
        from peft import LoraConfig, get_peft_model
        
        logger.info("üêå Initializing HuggingFace (slower but compatible)...")
        
        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # Load model
        self.training_model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            load_in_4bit=True,
            trust_remote_code=True
        )
        
        # Apply LoRA
        lora_config = LoraConfig(
            r=8,
            lora_alpha=32,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
            lora_dropout=0.1,
            bias="none",
            task_type="CAUSAL_LM",
        )
        self.training_model = get_peft_model(self.training_model, lora_config)
        
        # Ensure all LoRA parameters require gradients
        for name, param in self.training_model.named_parameters():
            if "lora_" in name or param.requires_grad:
                param.requires_grad = True
                logger.debug(f"Enabled gradients for: {name}")
        
        # Add value head
        self._add_value_head()
        
        # Create a wrapper model that includes both training_model and value_head
        self.model = self._create_combined_model()
        
        # CRITICAL: Ensure training model is in training mode and parameters require gradients
        self.training_model.train()
        self.value_head.train()
        
        # Double-check LoRA parameters still require gradients after all initialization
        lora_count = 0
        for name, param in self.training_model.named_parameters():
            if "lora_" in name:
                if not param.requires_grad:
                    logger.warning(f"‚ö†Ô∏è Forcing LoRA parameter {name} to require gradients")
                    param.requires_grad = True
                lora_count += 1
        logger.info(f"‚úÖ Verified {lora_count} LoRA parameters require gradients")
        
        logger.info("‚úÖ HuggingFace model initialized")
    
    def generate(self, prompts: List[str], max_tokens: int = 512, temperature: float = 0.1) -> List[str]:
        """Generate responses using vLLM or HuggingFace"""
        
        if self.use_vllm:
            return self._generate_vllm(prompts, max_tokens, temperature)
        else:
            return self._generate_hf(prompts, max_tokens, temperature)
    
    def _generate_vllm(self, prompts: List[str], max_tokens: int, temperature: float) -> List[str]:
        """Ultra-fast generation with vLLM"""
        start_time = time.time()
        
        sampling_params = SamplingParams(
            max_tokens=max_tokens,
            temperature=temperature,
            top_p=0.9,
            stop=["<|im_end|>", "<|endoftext|>"],
        )
        
        # Generate with vLLM
        outputs = self.vllm_engine.generate(prompts, sampling_params)
        
        # Extract generated text
        results = []
        for output in outputs:
            generated_text = output.outputs[0].text
            results.append(generated_text)
        
        generation_time = time.time() - start_time
        logger.info(f"üöÄ vLLM generated {len(prompts)} responses in {generation_time:.2f}s ({generation_time/len(prompts):.2f}s each)")
        
        return results
    
    def _generate_hf(self, prompts: List[str], max_tokens: int, temperature: float) -> List[str]:
        """Fallback generation with HuggingFace"""
        start_time = time.time()
        
        results = []
        for prompt in prompts:
            inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1500)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.training_model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    temperature=temperature,
                    do_sample=True,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.pad_token_id
                )
            
            generated_text = self.tokenizer.decode(outputs[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True)
            results.append(generated_text)
        
        generation_time = time.time() - start_time
        logger.info(f"üêå HuggingFace generated {len(prompts)} responses in {generation_time:.2f}s ({generation_time/len(prompts):.2f}s each)")
        
        return results
    
    
    def get_value(self, states: List[str]) -> torch.Tensor:
        """Compute state values for GRPO training"""
        values = []
        for state in states:
            tokens = self.tokenizer(state, return_tensors="pt", truncation=True, max_length=1500)
            tokens = {k: v.to(self.device) for k, v in tokens.items()}
            
            with torch.no_grad():
                hidden_states = self.training_model(**tokens, output_hidden_states=True).hidden_states[-1]
                # Use last token's hidden state
                last_hidden = hidden_states[:, -1, :]
                value = self.value_head(last_hidden)
                values.append(value.squeeze())
        
        return torch.stack(values)
    
    def parameters(self):
        """Get trainable parameters - specifically LoRA adapters + value head"""
        trainable_params = []
        
        # CRITICAL FIX: For PEFT models, we need to check if this is a PeftModel
        # and use the appropriate method to get trainable parameters
        try:
            from peft import PeftModel
            if isinstance(self.training_model, PeftModel):
                # For PEFT models, get only the trainable parameters
                for name, param in self.training_model.named_parameters():
                    if param.requires_grad and "lora" in name.lower():
                        trainable_params.append(param)
                        
                # Don't double-check base_model - PEFT already includes all trainable params
            else:
                # Regular model - get all parameters that require gradients
                for name, param in self.training_model.named_parameters():
                    if param.requires_grad:
                        trainable_params.append(param)
        except ImportError:
            # PEFT not available, use regular method
            for name, param in self.training_model.named_parameters():
                if param.requires_grad:
                    trainable_params.append(param)
        
        # Add value head parameters (these should always be trainable)
        for param in self.value_head.parameters():
            if not param.requires_grad:
                # Force value head parameters to require gradients
                param.requires_grad = True
            trainable_params.append(param)
        
        # Debug: Log the number of trainable parameters and their gradient status
        logger.info(f"üîß Found {len(trainable_params)} trainable parameters")
        if len(trainable_params) == 0:
            logger.error("‚ö†Ô∏è NO TRAINABLE PARAMETERS FOUND!")
            logger.error("Checking training_model state:")
            logger.error(f"  training_model.training: {self.training_model.training}")
            # List all parameters to debug
            for name, param in list(self.training_model.named_parameters())[:10]:
                logger.error(f"  {name}: requires_grad={param.requires_grad}, shape={param.shape}")
        
        return iter(trainable_params)  # Return iterator like nn.Module.parameters()
    
    def train(self):
        """Set to training mode"""
        self.training_model.train()
        self.value_head.train()
    
    def eval(self):
        """Set to evaluation mode"""
        self.training_model.eval()
        self.value_head.eval()


async def main():
    """Main training function with vLLM integration"""
    
    parser = argparse.ArgumentParser(description="vLLM-Enhanced GRPO Training")
    parser.add_argument("--config", type=str, default="training/configs/training_config_qwen3_0.6b.yaml")
    parser.add_argument("--device", type=str, default="cuda")
    parser.add_argument("--mixed-precision", type=str, default="fp16")
    parser.add_argument("--enable-profiling", action="store_true")
    args = parser.parse_args()
    
    logger.info("üöÄ Starting vLLM-Enhanced GRPO Training")
    logger.info(f"vLLM Available: {VLLM_AVAILABLE}")
    logger.info(f"Enable vLLM: {os.getenv('ENABLE_VLLM', 'false')}")
    
    # Initialize WandB
    try:
        import wandb
        wandb.init(
            project=os.getenv("WANDB_PROJECT", "multi-mcp-rl-vllm"),
            name=f"vllm-grpo-{time.strftime('%Y%m%d-%H%M%S')}",
            tags=["vllm", "grpo", "real-env"],
            config={
                "vllm_enabled": VLLM_AVAILABLE and os.getenv("ENABLE_VLLM", "false").lower() == "true",
                "device": args.device,
                "mixed_precision": args.mixed_precision
            }
        )
        logger.info("‚úÖ WandB initialized")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  WandB initialization failed: {e}")
    
    # Load configuration
    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)
    
    # Load training data
    data_path = config.get("data_path", "data/inputs/train.json")
    if not os.path.exists(data_path):
        data_path = "data/processed/train.json"
    
    with open(data_path, 'r') as f:
        training_data = json.load(f)
    
    logger.info(f"üìä Loaded {len(training_data)} training examples")
    
    # Initialize vLLM-enhanced policy  
    policy = VLLMQwenPolicy()
    
    # Debug: Check what attributes the policy object has
    logger.info(f"üîç Policy object type: {type(policy)}")
    logger.info(f"üîç Policy has get_trainable_parameters: {hasattr(policy, 'get_trainable_parameters')}")
    logger.info(f"üîç Policy methods: {[m for m in dir(policy) if not m.startswith('_') and callable(getattr(policy, m))]}")
    
    # Quick performance test first
    logger.info("üéØ Testing vLLM policy generation speed...")
    test_prompts = ["Hello, how are you today?", "What is machine learning?", "Explain quantum computing in simple terms."]
    start_time = time.time()
    responses = policy.generate(test_prompts, max_tokens=100, temperature=0.1)
    generation_time = time.time() - start_time
    
    logger.info(f"üöÄ Generated {len(test_prompts)} responses in {generation_time:.2f}s")
    logger.info(f"   Average time per response: {generation_time/len(test_prompts):.2f}s")
    for i, response in enumerate(responses):
        logger.info(f"Response {i+1}: {response[:100]}...")
    
    logger.info("‚úÖ vLLM integration verified - ready for full training")
    
    # NOW START THE ACTUAL TRAINING
    logger.info("üöÄ Starting full GRPO training with vLLM optimizations...")
    
    # Import required training components (using working imports from existing script)
    from training.core.grpo_trainer_gradient_fix import GRPOTrainerGradientFix
    from training.core.grpo_trainer import Trajectory
    from training.data.trajectory_collector import TrajectoryCollector, EpisodeResult
    from environments.simple_shared_manager import SimpleSharedManager
    from environments.mcp_tool_environment import MCPToolEnvironment
    
    # Setup environment factory function
    tool_manager = SimpleSharedManager()
    
    def env_factory(task_data):
        return MCPToolEnvironment(task_data=task_data)
    
    # Setup training config
    training_config = config.get("training", {})
    model_config = config.get("model", {})
    
    # Initialize reference policy (shared instance for memory efficiency)
    reference_policy = policy
    
    # Setup GRPO config
    grpo_config = {
        "learning_rate": float(training_config.get("learning_rate", 5e-6)),
        "batch_size": training_config.get("batch_size", 1),
        "kl_coeff": float(training_config.get("kl_coeff", 0.01)),
        "gamma": float(training_config.get("gamma", 0.99)),
        "lam": float(training_config.get("lam", 0.95)),
        "clip_ratio": float(training_config.get("clip_ratio", 0.2)),
        "value_loss_coeff": float(training_config.get("value_loss_coeff", 0.5)),
        "entropy_coeff": float(training_config.get("entropy_coeff", 0.01))
    }
    
    # Initialize GRPO trainer (using working trainer)
    trainer = GRPOTrainerGradientFix(
        policy=policy,
        reference_policy=reference_policy,
        grpo_config=grpo_config,
        training_config=training_config,
        device=torch.device(args.device),
        enable_mixed_precision=(args.mixed_precision == "fp16")
    )
    
    # Initialize trajectory collector
    collector = TrajectoryCollector(
        policy=policy,
        env_factory=env_factory,
        num_parallel_envs=training_config.get("num_workers", 1),  # Reduced for stability
        shared_tool_manager=tool_manager
    )
    
    logger.info("üéØ Starting training loop...")
    
    # Training loop
    max_epochs = training_config.get("max_epochs", 100)
    for epoch in range(max_epochs):
        logger.info(f"üìà Epoch {epoch+1}/{max_epochs}")
        
        # Collect episodes using real environment with vLLM speed
        start_time = time.time()
        episode_results = await collector.collect_batch(training_data[:4])  # Use small batch for efficiency
        collection_time = time.time() - start_time
        
        logger.info(f"‚ö° Collected {len(episode_results)} episodes in {collection_time:.2f}s")
        
        # Convert episodes to trajectories and train
        valid_episodes = [e for e in episode_results if e.is_valid()]
        if valid_episodes:
            # Convert episodes to trajectories for training
            trajectories = []
            for episode in valid_episodes:
                # Extract states, actions, rewards, and dones from episode trajectory
                states = []
                actions = []
                rewards = []
                dones = []
                
                for i, step in enumerate(episode.trajectory):
                    # Always append to maintain length consistency
                    states.append(step.get('state', {}))  # Default to empty dict
                    actions.append(step.get('action', ""))  # Default to empty string
                    rewards.append(step.get('reward', 0.0))  # Default to 0.0
                    dones.append(i == len(episode.trajectory) - 1)  # Last step is done
                
                # Ensure we have at least one step if trajectory is completely empty
                if not states:
                    states = [{}]  # Empty state
                    actions = [""]  # Empty action
                    rewards = [0.0]
                    dones = [True]
                
                # Final safety check - ensure all arrays have same length
                min_length = min(len(states), len(actions), len(rewards), len(dones))
                if min_length > 0:
                    states = states[:min_length]
                    actions = actions[:min_length]
                    rewards = rewards[:min_length]
                    dones = dones[:min_length]
                
                logger.info(f"üìä Trajectory lengths - States: {len(states)}, Actions: {len(actions)}, Rewards: {len(rewards)}, Dones: {len(dones)}")
                
                trajectory = Trajectory(
                    task_id=episode.task_id,
                    states=states,
                    actions=actions,
                    rewards=rewards,
                    dones=dones
                )
                trajectories.append(trajectory)
            
            # ROBUST WANDB LOGGING: Pre-update heartbeat to ensure visibility
            heartbeat_metrics = {
                "epoch": epoch,
                "heartbeat": 1,
                "collection_time": collection_time,
                "total_trajectories": len(trajectories),
                "vllm_enabled": policy.use_vllm,
                "status": "starting_training_step"
            }
            
            try:
                wandb.log(heartbeat_metrics)
                logger.debug("üì° Pre-training heartbeat logged to WandB")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  WandB heartbeat failed: {e}")
            
            start_time = time.time()
            metrics = trainer.train_step(trajectories)
            train_time = time.time() - start_time
            
            # Log comprehensive training metrics
            combined_metrics = {
                "epoch": epoch,
                "collection_time": collection_time,
                "train_time": train_time,
                "total_trajectories": len(trajectories),
                "vllm_enabled": policy.use_vllm,
                "status": "training_completed",
                **metrics
            }
            
            # Robust WandB logging with detailed error handling
            try:
                wandb.log(combined_metrics)
                logger.debug("üìä Training metrics logged to WandB successfully")
            except Exception as e:
                logger.error(f"üö® WandB metrics logging failed: {e}")
                logger.info(f"üìã Metrics that failed to log: {combined_metrics}")
                # Continue training even if logging fails
                
            logger.info(f"‚úÖ Training step completed in {train_time:.2f}s")
            logger.info(f"üìä Metrics: {metrics}")
        else:
            logger.warning("‚ö†Ô∏è  No trajectories collected, skipping training step")
        
        # Save checkpoint every 10 epochs
        if (epoch + 1) % 10 == 0:
            checkpoint_path = f"outputs/vllm-checkpoint-epoch-{epoch+1}.pt"
            policy.save_pretrained(checkpoint_path)
            logger.info(f"üíæ Saved checkpoint: {checkpoint_path}")
    
    logger.info("üéâ vLLM-enhanced training completed successfully!")
    
    # Final cleanup
    try:
        tool_manager.cleanup()
    except:
        pass


if __name__ == "__main__":
    asyncio.run(main())