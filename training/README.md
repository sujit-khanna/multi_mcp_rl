# SkyRL GRPO Training Module

This module provides complete Group Relative Policy Optimization (GRPO) training infrastructure for fine-tuning Qwen2.5-1.5B-Instruct on multi-turn tool use tasks.

## ğŸ“ Module Structure

```
training/
â”œâ”€â”€ __init__.py                 # Module initialization
â”œâ”€â”€ configs/                    # Configuration files
â”‚   â”œâ”€â”€ model_config.yaml       # Model and tokenizer settings
â”‚   â”œâ”€â”€ training_config.yaml    # Training hyperparameters
â”‚   â”œâ”€â”€ grpo_config.yaml        # GRPO algorithm settings
â”‚   â”œâ”€â”€ accelerate_config.yaml  # Multi-GPU training config
â”‚   â””â”€â”€ deepspeed_config.json   # DeepSpeed ZeRO-3 configuration
â”œâ”€â”€ core/                       # Core training components
â”‚   â”œâ”€â”€ qwen_policy.py          # Qwen model wrapper with policy methods
â”‚   â”œâ”€â”€ grpo_trainer.py         # Main GRPO training logic
â”‚   â”œâ”€â”€ trajectory_buffer.py    # Episode collection and buffering
â”‚   â””â”€â”€ distributed_utils.py    # Multi-GPU utilities
â”œâ”€â”€ data/                       # Data loading and processing
â”‚   â”œâ”€â”€ trajectory_collector.py # Collect rollouts from environment
â”‚   â”œâ”€â”€ data_loader.py          # Streaming data loader
â”‚   â””â”€â”€ preprocessing.py        # Data preprocessing utilities
â”œâ”€â”€ evaluation/                 # Model evaluation
â”‚   â”œâ”€â”€ online_evaluator.py     # Evaluate during training
â”‚   â””â”€â”€ checkpoint_evaluator.py # Evaluate saved checkpoints
â”œâ”€â”€ scripts/                    # Training and evaluation scripts
â”‚   â”œâ”€â”€ train_grpo.py           # Main training script
â”‚   â”œâ”€â”€ collect_trajectories.py # Collect episodes for analysis
â”‚   â”œâ”€â”€ evaluate_checkpoint.py  # Evaluate specific checkpoint
â”‚   â””â”€â”€ launch_distributed.py   # Launch distributed training
â””â”€â”€ utils/                      # Utility functions
    â”œâ”€â”€ logging_utils.py        # Logging and monitoring
    â”œâ”€â”€ checkpoint_utils.py     # Model checkpointing
    â””â”€â”€ monitoring.py           # Training metrics and monitoring
```

## ğŸ¯ Training Modes

### LoRA Mode (Development/Testing)
- **Hardware**: Single A100 40GB
- **Features**: 4-bit quantization + LoRA adapters
- **Use Case**: Fast iteration and development

### Full Fine-tuning Mode (Production)
- **Hardware**: 2x A100 40GB minimum
- **Features**: BF16 precision, DeepSpeed ZeRO-3
- **Use Case**: Production-quality model training

## ğŸš€ Key Features

### GRPO Algorithm Implementation
- **Group Relative Rewards**: Compare multiple rollouts per task
- **KL Divergence Penalty**: Prevent policy drift from reference model
- **GAE Advantage Estimation**: Gamma=0.99, Lambda=0.95
- **Reference Policy Updates**: Every 10k steps for stability

### Integration with Existing Infrastructure
- **MCPToolEnvironment**: Direct integration with SkyRL environment
- **Shared Tool Manager**: Single initialization across all processes
- **Async Tool Execution**: Proper handling of MCP async operations
- **Streaming Data**: Memory-efficient loading of large datasets

### Performance Optimizations
- **DeepSpeed ZeRO-3**: Distributed training with parameter offloading
- **Gradient Checkpointing**: Reduce memory usage during backprop
- **Flash Attention**: Efficient attention computation
- **Connection Pooling**: Reuse MCP server connections

## ğŸ“Š Expected Model Behavior

The trained model will produce structured outputs with reasoning and tool calls:

```
<think>
I need to find Apple's current stock price. I'll use the FMP financial tool to get a real-time quote for AAPL ticker symbol.
</think>

<tool_call>{"name": "fmp_get_quote", "arguments": {"symbol": "AAPL"}}</tool_call>
```

## ğŸ”§ Configuration

All configurations are in YAML/JSON format:
- **model_config.yaml**: Model architecture and generation settings
- **training_config.yaml**: Learning rates, batch sizes, optimization
- **grpo_config.yaml**: GRPO algorithm hyperparameters
- **accelerate_config.yaml**: Multi-GPU and distributed settings
- **deepspeed_config.json**: DeepSpeed ZeRO optimization

## ğŸ“ˆ Success Criteria

Training is considered successful when:
1. âœ… LoRA training runs without OOM on single GPU
2. âœ… Full fine-tuning works on 2x A100 with DeepSpeed
3. âœ… Model achieves >50% task completion rate
4. âœ… Training stability (KL divergence controlled, rewards increase)
5. âœ… Can process 100k+ tasks without memory issues

## ğŸ” Implementation Status

- [x] Directory structure created
- [x] Configuration templates ready
- [ ] Core GRPO trainer implementation
- [ ] Data loading and trajectory collection
- [ ] Model policy wrapper with log probability computation
- [ ] Evaluation and monitoring systems
- [ ] Training scripts and launchers

## ğŸ“ Next Steps

1. Implement `core/qwen_policy.py` with log probability computation
2. Build `core/grpo_trainer.py` with GRPO algorithm
3. Create `data/trajectory_collector.py` for episode collection
4. Implement evaluation pipeline
5. Create training scripts with proper error handling
6. Add comprehensive testing and smoke tests