{
  "success": false,
  "return_code": 1,
  "stdout": "\ud83d\udd25 GRPO Training Pipeline Smoke Test\nMode: lora\nSkip Model Tests: True\nTarget: Complete in under 2 minutes\n--------------------------------------------------\n\n============================================================\nSMOKE TEST RESULTS\n============================================================\nMode: lora\nDevice: cuda\nElapsed Time: 8.5 seconds\nSkip Model Tests: True\n\nTest Results:\n  policy_initialization: \u2705 PASS\n  data_loading: \u2705 PASS\n  trajectory_collection: \u2705 PASS\n  grpo_training: \u274c FAIL\n\nSummary: 3/4 tests passed\n\u26a0\ufe0f  Some tests failed. Check logs for details.\n============================================================\n\n\u274c Smoke test failed!\n",
  "stderr": "INFO:__main__:SmokeTest initialized: mode=lora, device=cuda\nINFO:__main__:macOS Unified Memory Available: 209.9 GB\nINFO:__main__:Starting comprehensive smoke tests...\nINFO:__main__:Using temporary directory: /tmp/grpo_smoke_test_wlgoofif\nINFO:__main__:GPU Memory: 0.00GB allocated, 0.00GB cached\nINFO:__main__:System Memory: 0.73GB RSS\nINFO:__main__:Testing policy initialization...\nINFO:core.qwen_policy:Loaded configuration from /tmp/grpo_smoke_test_wlgoofif/model_config.yaml\nINFO:core.qwen_policy:Loaded configuration from /tmp/grpo_smoke_test_wlgoofif/training_config.yaml\nINFO:core.qwen_policy:Loading model: Qwen/Qwen2.5-1.5B-Instruct\nINFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\nINFO:core.qwen_policy:LoRA applied: 1,777,664 trainable / 1,545,491,968 total parameters (0.12% trainable)\nINFO:core.qwen_policy:Model loaded successfully on cuda\nINFO:core.qwen_policy:Generation config setup complete with 5 stop tokens\nINFO:core.qwen_policy:QwenPolicy initialized successfully in LoRA mode\nINFO:__main__:Policy initialized: 1,777,664 trainable parameters\nINFO:core.qwen_policy:Loaded configuration from /tmp/grpo_smoke_test_wlgoofif/model_config.yaml\nINFO:core.qwen_policy:Loaded configuration from /tmp/grpo_smoke_test_wlgoofif/training_config.yaml\nINFO:core.qwen_policy:Loading model: Qwen/Qwen2.5-1.5B-Instruct\nINFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\nINFO:core.qwen_policy:LoRA applied: 1,777,664 trainable / 1,545,491,968 total parameters (0.12% trainable)\nINFO:core.qwen_policy:Model loaded successfully on cuda\nINFO:core.qwen_policy:Generation config setup complete with 5 stop tokens\nINFO:core.qwen_policy:QwenPolicy initialized successfully in LoRA mode\nINFO:__main__:GPU Memory: 5.76GB allocated, 5.92GB cached\nINFO:__main__:System Memory: 0.96GB RSS\nINFO:__main__:Testing data loading...\nINFO:data.data_loader:Building task index...\nINFO:data.data_loader:Task index built: 5 tasks after sharding\nINFO:data.data_loader:Complexity distribution: {'easy': 2, 'medium': 2, 'hard': 1}\nINFO:data.data_loader:StreamingDataset initialized: /tmp/grpo_smoke_test_wlgoofif/test_data.json (json), shard 0/1, 5 tasks\nINFO:data.data_loader:CurriculumSampler initialized: {'easy': 0.3, 'medium': 0.5, 'hard': 0.2} -> {'easy': 0.1, 'medium': 0.4, 'hard': 0.5}\nINFO:data.data_loader:TaskBatcher initialized: target_turns=8, max_batch=3\nINFO:__main__:Data loading test passed: 5 tasks, 1 in batch\nINFO:__main__:GPU Memory: 5.76GB allocated, 5.92GB cached\nINFO:__main__:System Memory: 0.96GB RSS\nINFO:__main__:Testing trajectory collection...\nINFO:__main__:Trajectory collection test passed: 2 trajectories\nINFO:__main__:GPU Memory: 5.76GB allocated, 5.92GB cached\nINFO:__main__:System Memory: 0.96GB RSS\nINFO:__main__:Testing GRPO training...\nINFO:core.qwen_policy:Loaded configuration from /tmp/grpo_smoke_test_wlgoofif/model_config.yaml\nINFO:core.qwen_policy:Loaded configuration from /tmp/grpo_smoke_test_wlgoofif/training_config.yaml\nINFO:core.qwen_policy:Loading model: Qwen/Qwen2.5-1.5B-Instruct\nINFO:core.qwen_policy:Configured 4-bit quantization for LoRA training\nINFO:core.qwen_policy:LoRA applied: 1,777,664 trainable / 890,394,112 total parameters (0.20% trainable)\nINFO:core.qwen_policy:Model loaded successfully on cuda\nINFO:core.qwen_policy:Generation config setup complete with 5 stop tokens\nINFO:core.qwen_policy:QwenPolicy initialized successfully in LoRA mode\nINFO:core.grpo_trainer:Model has 506 total parameters\nINFO:core.grpo_trainer:Found 168 trainable parameters\nINFO:core.grpo_trainer:Using AdamW optimizer with lr=0.0001 for 168 parameters\nINFO:core.grpo_trainer:Setup linear scheduler with 1 warmup steps\nINFO:core.grpo_trainer:GRPOTrainer initialized with 1,777,664 trainable parameters\nINFO:core.grpo_trainer:GRPO config: gamma=0.99, lambda=0.95, clip=0.2, kl_coef=0.1\nINFO:core.qwen_policy:Enabled gradients for 168 LoRA parameters\nINFO:__main__:Running training step 1/3...\nINFO:core.grpo_trainer:Starting train_step with 2 trajectories\nINFO:core.grpo_trainer:Trajectory 0: task_id=smoke_test_001, length=2, rewards=[0.5, 1.0], states_count=2, actions_count=2\nINFO:core.grpo_trainer:Trajectory 1: task_id=smoke_test_002, length=2, rewards=[0.5, 1.0], states_count=2, actions_count=2\nINFO:core.grpo_trainer:Moving 2 trajectories to device cuda\nERROR:core.grpo_trainer:\u274c Error in train_step: RuntimeError: Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment.  If you were attempting to deepcopy a module, this may be because of a torch.nn.utils.weight_norm usage, see https://github.com/pytorch/pytorch/pull/103001\nERROR:core.grpo_trainer:Train step error traceback:\nTraceback (most recent call last):\n  File \"/home/ubuntu/projects/multi_mcp_rl/training/core/grpo_trainer.py\", line 313, in train_step\n    trajectories = [traj.to_device(self.device) for traj in trajectories]\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/projects/multi_mcp_rl/training/core/grpo_trainer.py\", line 81, in to_device\n    new_traj = copy.deepcopy(self)\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/copy.py\", line 162, in deepcopy\n    y = _reconstruct(x, memo, *rv)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/copy.py\", line 259, in _reconstruct\n    state = deepcopy(state, memo)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/copy.py\", line 136, in deepcopy\n    y = copier(x, memo)\n        ^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/copy.py\", line 221, in _deepcopy_dict\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\n                             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/copy.py\", line 143, in deepcopy\n    y = copier(memo)\n        ^^^^^^^^^^^^\n  File \"/home/ubuntu/projects/multi_mcp_rl/.venv/lib/python3.12/site-packages/torch/_tensor.py\", line 136, in __deepcopy__\n    raise RuntimeError(\nRuntimeError: Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment.  If you were attempting to deepcopy a module, this may be because of a torch.nn.utils.weight_norm usage, see https://github.com/pytorch/pytorch/pull/103001\n\nERROR:__main__:GRPO training failed: Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment.  If you were attempting to deepcopy a module, this may be because of a torch.nn.utils.weight_norm usage, see https://github.com/pytorch/pytorch/pull/103001\nERROR:__main__:Traceback (most recent call last):\n  File \"/home/ubuntu/projects/multi_mcp_rl/training/tests/smoke_test.py\", line 514, in test_grpo_training\n    metrics = trainer.train_step(trajectories)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/projects/multi_mcp_rl/training/core/grpo_trainer.py\", line 313, in train_step\n    trajectories = [traj.to_device(self.device) for traj in trajectories]\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/projects/multi_mcp_rl/training/core/grpo_trainer.py\", line 81, in to_device\n    new_traj = copy.deepcopy(self)\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/copy.py\", line 162, in deepcopy\n    y = _reconstruct(x, memo, *rv)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/copy.py\", line 259, in _reconstruct\n    state = deepcopy(state, memo)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/copy.py\", line 136, in deepcopy\n    y = copier(x, memo)\n        ^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/copy.py\", line 221, in _deepcopy_dict\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\n                             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/copy.py\", line 143, in deepcopy\n    y = copier(memo)\n        ^^^^^^^^^^^^\n  File \"/home/ubuntu/projects/multi_mcp_rl/.venv/lib/python3.12/site-packages/torch/_tensor.py\", line 136, in __deepcopy__\n    raise RuntimeError(\nRuntimeError: Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment.  If you were attempting to deepcopy a module, this may be because of a torch.nn.utils.weight_norm usage, see https://github.com/pytorch/pytorch/pull/103001\n\nERROR:__main__:GRPO training failed, skipping checkpoint test\nINFO:__main__:Cleaned up temporary directory: /tmp/grpo_smoke_test_wlgoofif\n",
  "command": "/home/ubuntu/projects/multi_mcp_rl/.venv/bin/python /home/ubuntu/projects/multi_mcp_rl/training/tests/smoke_test.py --mode lora --skip_model_tests",
  "smoke_test_summary": "Smoke test execution failed",
  "test_name": "smoke_test_lora",
  "timestamp": 1754685588.6207259,
  "duration": 45.14854431152344
}