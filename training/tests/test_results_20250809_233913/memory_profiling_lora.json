{
  "success": true,
  "return_code": 0,
  "stdout": "\ud83d\udd0d GRPO Memory Profiler\nMode: lora\nSequence lengths: [512, 1024, 2048]\nMax batch size: 8\nMax gradient accumulation: 16\n--------------------------------------------------\n\u26a1 Quick mode enabled - testing fewer configurations\n\n================================================================================\nMEMORY PROFILING RESULTS\n================================================================================\nMode: lora\nDevice: cuda\nTotal System Memory: 216.3 GB\nBaseline Memory: 0.00 GB\n\nConfigurations Tested: 2\nSuccessful: 0\nFailed: 2\n\n\u274c FAILED CONFIGURATIONS: 2\n  SeqLen=512, Batch=1: Runtime error: Only Tensors created explicitly by \n  SeqLen=1024, Batch=1: Runtime error: Only Tensors created explicitly by \n================================================================================\n\n\u2705 Memory profiling completed!\n\ud83d\udcc4 Results saved to: memory_profile_lora.json\n",
  "stderr": "INFO:__main__:MemoryProfiler initialized: mode=lora, device=cuda\nINFO:__main__:Baseline memory usage: 0.00 GB\nINFO:__main__:Total system memory: 216.3 GB\nINFO:__main__:Starting comprehensive memory profiling...\nINFO:__main__:Using temporary directory: /tmp/grpo_memory_profile_f03i3ja0\nINFO:__main__:Phase 1: Sequence length sweep\nINFO:__main__:Running sequence length sweep: lengths=[512, 1024], batch=1, grad_accum=1\nINFO:__main__:Profiling: seq_len=512, batch=1, grad_accum=1\nINFO:__main__:  Loading model...\nINFO:core.qwen_policy:Loaded configuration from /tmp/grpo_memory_profile_f03i3ja0/model_config.yaml\nINFO:core.qwen_policy:Loaded configuration from /tmp/grpo_memory_profile_f03i3ja0/training_config.yaml\nINFO:core.qwen_policy:Loading model: Qwen/Qwen2.5-1.5B-Instruct\nINFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\nINFO:core.qwen_policy:LoRA applied: 18,464,768 trainable / 1,562,179,072 total parameters (1.18% trainable)\nINFO:core.qwen_policy:Model loaded successfully on cuda\nINFO:core.qwen_policy:Generation config setup complete with 5 stop tokens\nINFO:core.qwen_policy:QwenPolicy initialized successfully in LoRA mode\nINFO:__main__:  Creating reference policy...\nINFO:core.qwen_policy:Loaded configuration from /tmp/grpo_memory_profile_f03i3ja0/model_config.yaml\nINFO:core.qwen_policy:Loaded configuration from /tmp/grpo_memory_profile_f03i3ja0/training_config.yaml\nINFO:core.qwen_policy:Loading model: Qwen/Qwen2.5-1.5B-Instruct\nINFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\nINFO:core.qwen_policy:LoRA applied: 18,464,768 trainable / 1,562,179,072 total parameters (1.18% trainable)\nINFO:core.qwen_policy:Model loaded successfully on cuda\nINFO:core.qwen_policy:Generation config setup complete with 5 stop tokens\nINFO:core.qwen_policy:QwenPolicy initialized successfully in LoRA mode\nINFO:__main__:  Creating trainer...\nINFO:core.grpo_trainer:Model has 730 total parameters\nINFO:core.grpo_trainer:Found 392 trainable parameters\nINFO:core.grpo_trainer:Using AdamW optimizer with lr=0.0002 for 392 parameters\nINFO:core.grpo_trainer:Setup cosine scheduler with 100 warmup steps\nINFO:core.grpo_trainer:GRPOTrainer initialized with 18,464,768 trainable parameters\nINFO:core.grpo_trainer:GRPO config: gamma=0.99, lambda=0.95, clip=0.2, kl_coef=0.1\nINFO:__main__:  Running 3 training steps...\nINFO:core.qwen_policy:Enabled gradients for 392 LoRA parameters\nWARNING:core.qwen_policy:Empty or invalid messages: []\nINFO:core.grpo_trainer:Starting train_step with 1 trajectories\nINFO:core.grpo_trainer:Trajectory 0: task_id=memory_test_000, length=2, rewards=[0.5, 1.0], states_count=2, actions_count=2\nINFO:core.grpo_trainer:Moving 1 trajectories to device cuda\nERROR:core.grpo_trainer:\u274c Error in train_step: RuntimeError: Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment.  If you were attempting to deepcopy a module, this may be because of a torch.nn.utils.weight_norm usage, see https://github.com/pytorch/pytorch/pull/103001\nERROR:core.grpo_trainer:Train step error traceback:\nTraceback (most recent call last):\n  File \"/home/ubuntu/multi_mcp_rl/training/core/grpo_trainer.py\", line 313, in train_step\n    trajectories = [traj.to_device(self.device) for traj in trajectories]\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/multi_mcp_rl/training/core/grpo_trainer.py\", line 81, in to_device\n    new_traj = copy.deepcopy(self)\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/copy.py\", line 162, in deepcopy\n    y = _reconstruct(x, memo, *rv)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/copy.py\", line 259, in _reconstruct\n    state = deepcopy(state, memo)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/copy.py\", line 136, in deepcopy\n    y = copier(x, memo)\n        ^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/copy.py\", line 221, in _deepcopy_dict\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\n                             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/copy.py\", line 143, in deepcopy\n    y = copier(memo)\n        ^^^^^^^^^^^^\n  File \"/home/ubuntu/multi_mcp_rl/.venv/lib/python3.12/site-packages/torch/_tensor.py\", line 136, in __deepcopy__\n    raise RuntimeError(\nRuntimeError: Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment.  If you were attempting to deepcopy a module, this may be because of a torch.nn.utils.weight_norm usage, see https://github.com/pytorch/pytorch/pull/103001\n\nERROR:__main__:  \u274c Runtime error: Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment.  If you were attempting to deepcopy a module, this may be because of a torch.nn.utils.weight_norm usage, see https://github.com/pytorch/pytorch/pull/103001\nINFO:__main__:Profiling: seq_len=1024, batch=1, grad_accum=1\nINFO:__main__:  Loading model...\nINFO:core.qwen_policy:Loaded configuration from /tmp/grpo_memory_profile_f03i3ja0/model_config.yaml\nINFO:core.qwen_policy:Loaded configuration from /tmp/grpo_memory_profile_f03i3ja0/training_config.yaml\nINFO:core.qwen_policy:Loading model: Qwen/Qwen2.5-1.5B-Instruct\nINFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\nINFO:core.qwen_policy:LoRA applied: 18,464,768 trainable / 1,562,179,072 total parameters (1.18% trainable)\nINFO:core.qwen_policy:Model loaded successfully on cuda\nINFO:core.qwen_policy:Generation config setup complete with 5 stop tokens\nINFO:core.qwen_policy:QwenPolicy initialized successfully in LoRA mode\nINFO:__main__:  Creating reference policy...\nINFO:core.qwen_policy:Loaded configuration from /tmp/grpo_memory_profile_f03i3ja0/model_config.yaml\nINFO:core.qwen_policy:Loaded configuration from /tmp/grpo_memory_profile_f03i3ja0/training_config.yaml\nINFO:core.qwen_policy:Loading model: Qwen/Qwen2.5-1.5B-Instruct\nINFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\nINFO:core.qwen_policy:LoRA applied: 18,464,768 trainable / 1,562,179,072 total parameters (1.18% trainable)\nINFO:core.qwen_policy:Model loaded successfully on cuda\nINFO:core.qwen_policy:Generation config setup complete with 5 stop tokens\nINFO:core.qwen_policy:QwenPolicy initialized successfully in LoRA mode\nINFO:__main__:  Creating trainer...\nINFO:core.grpo_trainer:Model has 730 total parameters\nINFO:core.grpo_trainer:Found 392 trainable parameters\nINFO:core.grpo_trainer:Using AdamW optimizer with lr=0.0002 for 392 parameters\nINFO:core.grpo_trainer:Setup cosine scheduler with 100 warmup steps\nINFO:core.grpo_trainer:GRPOTrainer initialized with 18,464,768 trainable parameters\nINFO:core.grpo_trainer:GRPO config: gamma=0.99, lambda=0.95, clip=0.2, kl_coef=0.1\nINFO:__main__:  Running 3 training steps...\nINFO:core.qwen_policy:Enabled gradients for 392 LoRA parameters\nWARNING:core.qwen_policy:Empty or invalid messages: []\nINFO:core.grpo_trainer:Starting train_step with 1 trajectories\nINFO:core.grpo_trainer:Trajectory 0: task_id=memory_test_000, length=2, rewards=[0.5, 1.0], states_count=2, actions_count=2\nINFO:core.grpo_trainer:Moving 1 trajectories to device cuda\nERROR:core.grpo_trainer:\u274c Error in train_step: RuntimeError: Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment.  If you were attempting to deepcopy a module, this may be because of a torch.nn.utils.weight_norm usage, see https://github.com/pytorch/pytorch/pull/103001\nERROR:core.grpo_trainer:Train step error traceback:\nTraceback (most recent call last):\n  File \"/home/ubuntu/multi_mcp_rl/training/core/grpo_trainer.py\", line 313, in train_step\n    trajectories = [traj.to_device(self.device) for traj in trajectories]\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/multi_mcp_rl/training/core/grpo_trainer.py\", line 81, in to_device\n    new_traj = copy.deepcopy(self)\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/copy.py\", line 162, in deepcopy\n    y = _reconstruct(x, memo, *rv)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/copy.py\", line 259, in _reconstruct\n    state = deepcopy(state, memo)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/copy.py\", line 136, in deepcopy\n    y = copier(x, memo)\n        ^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/copy.py\", line 221, in _deepcopy_dict\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\n                             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/copy.py\", line 143, in deepcopy\n    y = copier(memo)\n        ^^^^^^^^^^^^\n  File \"/home/ubuntu/multi_mcp_rl/.venv/lib/python3.12/site-packages/torch/_tensor.py\", line 136, in __deepcopy__\n    raise RuntimeError(\nRuntimeError: Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment.  If you were attempting to deepcopy a module, this may be because of a torch.nn.utils.weight_norm usage, see https://github.com/pytorch/pytorch/pull/103001\n\nERROR:__main__:  \u274c Runtime error: Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment.  If you were attempting to deepcopy a module, this may be because of a torch.nn.utils.weight_norm usage, see https://github.com/pytorch/pytorch/pull/103001\nERROR:__main__:No viable sequence lengths found!\nINFO:__main__:Cleaned up temporary directory: /tmp/grpo_memory_profile_f03i3ja0\nINFO:__main__:Results saved to memory_profile_lora.json\n",
  "command": "/home/ubuntu/multi_mcp_rl/.venv/bin/python /home/ubuntu/multi_mcp_rl/training/tests/memory_profile.py --mode lora --quick",
  "memory_profile_data": {
    "metadata": {
      "mode": "lora",
      "device": "cuda",
      "timestamp": 1754782785.0241053,
      "total_system_memory_gb": 216.2584228515625,
      "baseline_memory_gb": 0.0
    },
    "results": [
      {
        "sequence_length": 512,
        "batch_size": 1,
        "gradient_accumulation_steps": 1,
        "mode": "lora",
        "device": "cuda",
        "start_memory_gb": 0.0,
        "success": false,
        "error": "Runtime error: Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment.  If you were attempting to deepcopy a module, this may be because of a torch.nn.utils.weight_norm usage, see https://github.com/pytorch/pytorch/pull/103001",
        "peak_memory_gb": 0.0,
        "model_memory_gb": 5.826162815093994,
        "training_memory_gb": 0.0,
        "memory_per_sample": 0.0,
        "recommended_batch_size": 0
      },
      {
        "sequence_length": 1024,
        "batch_size": 1,
        "gradient_accumulation_steps": 1,
        "mode": "lora",
        "device": "cuda",
        "start_memory_gb": 0.008056640625,
        "success": false,
        "error": "Runtime error: Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment.  If you were attempting to deepcopy a module, this may be because of a torch.nn.utils.weight_norm usage, see https://github.com/pytorch/pytorch/pull/103001",
        "peak_memory_gb": 0.0,
        "model_memory_gb": 5.824697971343994,
        "training_memory_gb": 0.0,
        "memory_per_sample": 0.0,
        "recommended_batch_size": 0
      }
    ],
    "recommendations": {
      "error": "No successful configurations found"
    }
  },
  "successful_configurations": 0,
  "total_configurations_tested": 2,
  "test_name": "memory_profiling_lora",
  "timestamp": 1754782786.13137,
  "duration": 32.72900915145874
}